{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2XGJL/oKDc0ckX+0P+B0A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maha3069/youtube_tuto/blob/main/tuto_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langchain"
      ],
      "metadata": {
        "id": "YDYw7Pfq8Ggk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##installation"
      ],
      "metadata": {
        "id": "HcID9tvl8tLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq\n",
        "!pip install langchain-groq\n",
        "!pip install langchain-huggingface\n",
        "!pip install langchain-text-splitters\n",
        "!pip install langchain-community\n",
        "!pip install chromadb\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ds7NpwH282Bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Integration des llms"
      ],
      "metadata": {
        "id": "_t1YY4KX9ivE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Prompt to enter the Groq API key securely (won't show on screen)\n",
        "os.environ[\"GROQ_API_KEY\"] = getpass(\"Enter your Groq API key: \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOViy9sb-U6z",
        "outputId": "b1a929f5-49d1-4c97-8a44-65daf789c0bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "llm=ChatGroq(model=\"openai/gpt-oss-20b\")\n",
        "response = llm.invoke(\"salut\")\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Aa501YL8sJ8",
        "outputId": "20db4a78-7a87-4378-eb3b-d6f684c96173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Salut ! Comment ça va aujourd’hui ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt"
      ],
      "metadata": {
        "id": "huSciAozXULj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "c'est le prompt alors pour avoir des prompt plus avances langcahin offre des  promptemplates qui permettent de personaliser les prompt.Donc les promptemplates ces sont des composant pour generer des prompt  en introduire de variables , des places holders extra\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gBkLLJ4JH6Gp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ly-ng_jG8F0i",
        "outputId": "1851735a-cfed-4193-e6d4-a661d9f69e19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traduire la phrase suivante\"Bonjour comment ca va\" en Anglais\n"
          ]
        }
      ],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate.from_template(\"Traduire la phrase suivante\\\"{phrase}\\\" en {langue}\")\n",
        "prompt_formatted = prompt.format(phrase = \"Bonjour comment ca va\", langue =\"Anglais\")\n",
        "print(prompt_formatted)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm.invoke(prompt_formatted).content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uXfdPjAJeKB",
        "outputId": "567aa116-9105-4e3a-8b8f-e228e1dc9ed6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**English translation:**  \n",
            "\"Hello, how are you?\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les autres  qui sont interrissantes :\n",
        "\n",
        "ChatPromptTemplate\tTemplate qui génère une liste de messages structurés avec rôles (system, human, assistant, etc).\tPour les modèles chat, avec instructions système et user.\n",
        "\n",
        "MessagesPlaceholder\tPermet d’insérer dynamiquement une liste de messages (ex: historique de conversation).\tGérer l’historique ou insérer des messages variables."
      ],
      "metadata": {
        "id": "R-xJtK_ZTLLs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connecter les donnes\n"
      ],
      "metadata": {
        "id": "RAiiJX9_NJHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chargement"
      ],
      "metadata": {
        "id": "qT9tOz68WFlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader('/content/histoirep.txt')\n",
        "docs = loader.load()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rzLHkUVcV8-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transfromation:Chunking"
      ],
      "metadata": {
        "id": "Dp1nPeVrXOZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "spliter = RecursiveCharacterTextSplitter(chunk_size=200,chunk_overlap=20)\n",
        "chunks = spliter.split_documents(docs)\n",
        "print(\"len chunks\", len(chunks))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gcwBeM6WTJL",
        "outputId": "2e7ec8aa-a134-47e6-b794-ba04e9420ff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len chunks 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Embedding\n",
        "\n"
      ],
      "metadata": {
        "id": "kT32G3aAXQ-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt to enter the Groq API key securely (won't show on screen)\n",
        "import os\n",
        "from getpass import getpass\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass(\"Enter your HF API key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPqXMnVbX9M0",
        "outputId": "7f794301-2e7f-461e-b094-68e520d1a927"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your HF API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "TXvrgxK6XNBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BdVectorielle"
      ],
      "metadata": {
        "id": "ZmBLiWFod6nE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "vectorstore = Chroma.from_documents(chunks, embedding_model, collection_name=\"histoire_collection\")\n",
        "print(\"Données enregistrées dans la base vectorielle Chroma.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YKcXkJ_dF-y",
        "outputId": "a281fd68-0177-48d5-a340-4302ad41946a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Données enregistrées dans la base vectorielle Chroma.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"qui est Sophie?\"\n",
        "docs = vectorstore.similarity_search(query)\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "mN7yRf8Ld_fk",
        "outputId": "9a78a2ae-8120-458f-b226-667560b18b1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "À Paris, dans le quartier animé du Marais, le Café des Lilas est un petit bistrot où les habitués se retrouvent. Chaque matin, Sophie, une libraire passionnée de 32 ans, s'installe à une table près de\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##chains"
      ],
      "metadata": {
        "id": "hwLLwtyqX5I2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "chain.run(phrase=\"Bonjour comment ca va\", langue=\"Anglais\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "I47vrMHoX7bs",
        "outputId": "49976c4e-fb7b-4355-c27c-10999e103caf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello, how are you?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **SimpleSequentialChain** : relie plusieurs chaînes de façon linéaire (ex. un LLM qui rédige → un LLM qui traduit).\n",
        "- **SequentialChain** (avancé) : permet de composer plusieurs étapes en gérant les variables de sortie/entrée entre elles."
      ],
      "metadata": {
        "id": "Pxd0mqc-AoFx"
      }
    }
  ]
}